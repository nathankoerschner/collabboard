# CollabBoard Writeup

## AI Development Log

### Tools & Workflow

I utilized Claude Code extensively--for the initial build, for new features and adjustments, and for all UI changes.
I also utilized Codex for bug resolution (using a TDD pattern I'll explain below).
Furthermore I used ChatGPT chat in browser for research.

### MCP Usage

I didn't use any MCP servers.

### Effective Prompts

I found the following prompt very successful (will explain below my strategy for spec-driven development).

```
Read this plan file @spec.md and interview me in detail using the AskUserQuestionTool about
literally anything: technical implementation, UI & UX, concerns, tradeoffs, etc.
but make sure the questions are not obvious.

Be very in-depth and continue interviewing me continually until it’s complete, then write the spec to the file.
```

I found queries along the lines of the following to be very helpful for generating tests:

```
I'm noticing <bug>. Create a failing, non-deterministic test that demonstrates <bug description>. Make sure that it's a failing test, and be sure not to hard-code it's outcome in any way.
```

### Code Analysis

100% of my code was generated by LLM's.

### Strengths & Limitations

AI really excelled in creating new features. It struggled with catching edge cases around features that have a lot of "logic under the hood"--for example, my undo/redo implementation.

### Key Learnings

#### Spec driven Development

The key thing that I found to be very helpful throughout this process was "spec driven devleopment."
My main workflow around this involved first giving a high-level overview of the spec in a markdown document, then using the `/interview` skill I have in Claude Code (uses the same interview prompt shared above) in order to have Claude gather details from me about the spec.
After doing this, I would clear context and have Claude make a plan to impiment the document.
I found this especially helpful in creating the MVP--basically by following this process, I was able to get all of the initial features working for the MVP in a single prompt (after plenty of presearch and time spent in the interview with Claude).

#### Test driven development

The next pattern I found incredibly helpful was to use test driven development in order to solve bugs.
I found this especially effective since both creating a failing test and then fixing the code such that the test passed were two things that the LLM could do in an unsupervised way.
I found that I trusted Codex much more than Claude Code in doing these bug related fixes and test generations.

#### Context clearing

I found that it is very helpful to, whenever possible, clear the context of the chat and start from zero. This is consistent with recent research I've been seeing around prompting strategies (including some research that indicates that LLM's greatly degrade in performance from ordinary chat-style interaction vs one shot prompting).
I've taken to aiming to "one shot" most issues, giving the LLM clean context before prompting.

#### Ability to work on many things at once--polish while the LLM is working on big features

When I started this project I thought that I would work on the big features incrementally, building up to the final build as we went through the week, and ending the week with final polish. But I found myself with a lot of spare time when the LLM was working on the bigger features--so I had plenty of time to work on polish on my own time.
This is basically shifting my understanding of how to go about working alongside LLM's.

---

## AI Cost Analysis

### Development & Testing Costs

I am on the $100/month Anthropic subscription, and the $20/month OpenAI subscription.
To generate my costs I used this command:
`npx ccusage daily --since 20260215 --breakdown --json`
and then asked Claude to summarize this as a markdown table:

#### Daily Usage

| Date      |     Input |      Output |  Cache Create |      Cache Read |    Total Tokens |        Cost |
| --------- | --------: | ----------: | ------------: | --------------: | --------------: | ----------: |
| Feb 16    |     2,093 |      24,849 |       836,923 |      31,170,799 |      32,034,664 |      $21.25 |
| Feb 17    |       846 |      25,044 |       874,515 |      31,367,991 |      32,268,396 |      $21.08 |
| Feb 18    |     1,898 |      32,343 |     2,339,405 |      77,531,030 |      79,904,676 |      $52.62 |
| Feb 19    |     2,793 |      36,968 |     2,296,506 |      88,046,184 |      90,382,451 |      $58.34 |
| **Total** | **7,630** | **119,204** | **6,347,349** | **228,116,004** | **234,590,187** | **$153.29** |

#### Cost by Model

| Model      | Feb 16 | Feb 17 | Feb 18 | Feb 19 |   Total |
| ---------- | -----: | -----: | -----: | -----: | ------: |
| Opus 4.6   | $21.21 | $20.68 | $51.57 | $57.32 | $150.78 |
| Sonnet 4.6 |      — |  $0.27 |  $0.78 |  $0.93 |   $1.98 |
| Haiku 4.5  |  $0.05 |  $0.13 |  $0.27 |  $0.09 |   $0.54 |

Opus dominates at ~98% of total spend. Cache reads account for 97% of all tokens.

### Production Cost Projections

Assuming users interact with the app twice a business day and use the AI feature an average of six times a visit, that's roughly 12x5x4 = 240 interactions per user per month.
At an upper bound of $0.0012 per query (roughly the cost to run a query that generates a SWOT analysis, one of the more complicated things), that gives us the following costs:

| 100 Users    | 1,000 Users | 10,000 Users | 100,000 Users |
| ------------ | ----------- | ------------ | ------------- |
| $28.80/month | $288/month  | $2,880/month | $28,800/month |
