# CollabBoard Writeup

## AI Development Log

### Tools & Workflow
I utilized Claude Code extensively--for the initial build, for new features and adjustments, and for all UI changes.
I also utilized Codex for bug resolution (using a TDD pattern I'll explain below).
Furthermore I used ChatGPT chat in browser for research.

### MCP Usage
I didn't use any MCP servers.

### Effective Prompts
I found the following prompt very successful (will explain below my strategy for spec-driven development).
```
Read this plan file @spec.md and interview me in detail using the AskUserQuestionTool about 
literally anything: technical implementation, UI & UX, concerns, tradeoffs, etc. 
but make sure the questions are not obvious.

Be very in-depth and continue interviewing me continually until itâ€™s complete, then write the spec to the file.
```

I found queries along the lines of the following to be very helpful for generating tests:
```
I'm noticing <bug>. Create a failing, non-deterministic test that demonstrates <bug description>. Make sure that it's a failing test, and be sure not to hard-code it's outcome in any way.
```

### Code Analysis
100% of my code was generated by LLM's.

### Strengths & Limitations
AI really excelled in creating new features. It struggled with catching edge cases around features that have a lot of "logic under the hood"--for example, my undo/redo implementation.

### Key Learnings
- Spec driven Development
- Test driven development
- Context clearing
- Asking for understanding
- Ability to work on many things at once--polish while the LLM is working on big features

---

## AI Cost Analysis

### Development & Testing Costs

- LLM API costs (OpenAI, Anthropic, etc.)
- Total tokens consumed (input/output breakdown)
- Number of API calls made
- Any other AI-related costs (embeddings, hosting, etc.)

### Production Cost Projections

| 100 Users | 1,000 Users | 10,000 Users | 100,000 Users |
|---|---|---|---|
| $___/month | $___/month | $___/month | $___/month |

Include assumptions: average AI commands per user per session, average sessions per user per month, token counts per command type.

