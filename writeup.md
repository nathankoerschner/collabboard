# CollabBoard Writeup

## AI Development Log

### Tools & Workflow

I utilized Claude Code extensively--for the initial build, for new features and adjustments, and for all UI changes.
I also utilized Codex for bug resolution (using a TDD pattern I'll explain below).
Furthermore I used ChatGPT chat in browser for research.

### MCP Usage

I didn't use any MCP servers.

### Effective Prompts

I found the following prompt very successful (will explain below my strategy for spec-driven development).

```
Read this plan file @spec.md and interview me in detail using the AskUserQuestionTool about
literally anything: technical implementation, UI & UX, concerns, tradeoffs, etc.
but make sure the questions are not obvious.

Be very in-depth and continue interviewing me continually until itâ€™s complete, then write the spec to the file.
```

I found queries along the lines of the following to be very helpful for generating tests:

```
I'm noticing <bug>. Create a failing, non-deterministic test that demonstrates <bug description>. Make sure that it's a failing test, and be sure not to hard-code it's outcome in any way.
```

This is a query that I often use now for handing off work to new model instances or between Claude and Codex:

```
Can you write a problem statement and declaration of the next step that needs to happen here?
```

### Code Analysis

The vast majority of my code was generated by LLM's. The main exception is the system prompts for the AI agent, which were largely written by hand.

### Strengths & Limitations

AI really excelled in creating new features. It struggled with catching edge cases around features that have a lot of "logic under the hood"--for example, my undo/redo implementation.

### Key Learnings

#### Spec driven Development

The key thing that I found to be very helpful throughout this process was "spec driven devleopment."
My main workflow around this involved first giving a high-level overview of the spec in a markdown document, then using the `/interview` skill I have in Claude Code (uses the same interview prompt shared above) in order to have Claude gather details from me about the spec.
After doing this, I would clear context and have Claude make a plan to impiment the document.
I found this especially helpful in creating the MVP--basically by following this process, I was able to get all of the initial features working for the MVP in a single prompt (after plenty of presearch and time spent in the interview with Claude).

#### Test driven development

The next pattern I found incredibly helpful was to use test driven development in order to solve bugs.
I found this especially effective since both creating a failing test and then fixing the code such that the test passed were two things that the LLM could do in an unsupervised way.
I found that I trusted Codex much more than Claude Code in doing these bug related fixes and test generations.

#### Context clearing

I found that it is very helpful to, whenever possible, clear the context of the chat and start from zero. This is consistent with recent research I've been seeing around prompting strategies (including some research that indicates that LLM's greatly degrade in performance from ordinary chat-style interaction vs one shot prompting).
I've taken to aiming to "one shot" most issues, giving the LLM clean context before prompting.

#### Ability to work on many things at once--polish while the LLM is working on big features

When I started this project I thought that I would work on the big features incrementally, building up to the final build as we went through the week, and ending the week with final polish. But I found myself with a lot of spare time when the LLM was working on the bigger features--so I had plenty of time to work on polish on my own time.
This is basically shifting my understanding of how to go about working alongside LLM's.

---

## AI Cost Analysis

### Development & Testing Costs

These costs were very minimal.
Here's the breakdown:

| Metric         | Value     |
| -------------- | --------- |
| Total Spend    | $3.62     |
| Total Tokens   | 2,373,052 |
| Total Requests | 1,333     |

### Production Cost Projections

There are two AI cost drivers:

1. **Board AI queries**: Assuming users interact with the app twice a business day and use the AI feature an average of six times a visit, that's roughly 12x5x4 = 240 interactions per user per month. At an upper bound of $0.0012 per query (roughly the cost to run a query that generates a SWOT analysis, one of the more complicated things), that's **$0.288/user/month**.

2. **GitHub integration queries**: These are heavier (~9,000 tokens, ~$0.007 per query). Assuming a user triggers this roughly once per business day, that's 20 queries/month at $0.007 = **$0.14/user/month**.

**Total per user: ~$0.43/month**

| 100 Users    | 1,000 Users | 10,000 Users | 100,000 Users |
| ------------ | ----------- | ------------ | ------------- |
| $42.80/month | $428/month  | $4,280/month | $42,800/month |
